---
title: "Team_4_Project_DDY"
author: "Dingyi Duan"
date: "6/7/2022"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load the data

```{r}

library(caret)
# load the dataset
OnlineNewsPopularity <- read.csv("https://raw.githubusercontent.com/dingyiduan7/ADS-503-Applied-Predictive-Modeling/main/Datasets/OnlineNewsPopularity.csv")
dim(OnlineNewsPopularity)
summary(OnlineNewsPopularity)

```
```{r}

# url doesn't contribute to predictions

df <- OnlineNewsPopularity[, -1]

X <- df[ , -which(names(df) %in% c("shares"))]
y <- df$shares


```

# EDA

```{r}

# check for missing values
col_na <-sort(colSums(is.na(X)))
barplot(sort(col_na, decreasing = TRUE), las=2, cex.names = 0.55, main = "NA by predictors")

```
No missing values. No need for imputation.

```{r}

# check for degenerate features using zero variance
degen.cols <- nearZeroVar(X)

# plot the degenerate columns
for (i in degen.cols){
  
  barplot(table(X[, c(i)]),
          main = paste("Distribution of", colnames(X)[i]),
          xlab=colnames(X)[i],
          ylab="Frequency")
}

```
One predictors is degenerate predictor: kw_min_max.

```{r}

# use barplot to check for frequency distributions for the categorical predictors
for (i in 1:ncol(df)){
  
  barplot(table(df[, c(i)]),
          main = paste("Distribution of", colnames(df)[i]),
          xlab=colnames(df)[i],
          ylab="Frequency")
}

```
Many Predictors have skewness, need to perform required transformation before training.

# Preprocessing
```{r}

# remove zero variance predictors
X <- X[, -degen.cols]

```

```{r}

# separate binary predictors and continuous predictors
bi_var <- Filter(function(x) all(x %in% c(0, 1)), X)

con_var <- X[, !names(X) %in% names(bi_var)]

```

```{r}

# calculate colinearity between binary variables
corr_bi <- cor(bi_var)
library(corrplot)
corrplot(corr_bi, order = "hclust")

# find highly correlated predictors, using cutoff value of 0.70
highCorr_bi <- findCorrelation(corr_bi, cutoff = .7)

```

```{r}

# remove highly correlated binary variables
filtered_bi <- bi_var[, -highCorr_bi] # remove highly correlated predictors
corr_bi2 <- cor(filtered_bi)
corrplot(corr_bi2, order = "hclust")

highCorr_bi2 <- findCorrelation(cor(filtered_bi), cutoff = .70) # check for highly correlated predictors again

```

```{r}

# calculate colinearity between continuous variables
corr_con <- cor(con_var)
corrplot(corr_con, order = "hclust")

# find highly correlated predictors, using cutoff value of 0.70
highCorr_con <- findCorrelation(corr_con, cutoff = .7)

```

```{r}

# remove highly correlated continuous variables
filtered_con <- con_var[, -highCorr_con] # remove highly correlated predictors
corr_con2 <- cor(filtered_con)
corrplot(corr_con2, order = "hclust")

highCorr_con2 <- findCorrelation(cor(filtered_con), cutoff = .70) # check for highly correlated predictors again

```
```{r}

# merge binary and continuous variables back together 

df <- data.frame(filtered_bi, filtered_con, y)

# to avoid perfect multicollinearity, we remove the dummy variables of weekday_is_sunday

df <- df[ , -which(names(df) %in% c("weekday_is_sunday"))]

```

```{r}

set.seed(100) # repeat the results

# split the train and test to 0.8/0.2
trainingRows <- createDataPartition(df$y, p = .80,  list = FALSE)

train <- df[trainingRows, ]
test <- df[-trainingRows, ]

train_X <- train[,-46]
train_y <- train$y

test_X <- test[,-46]
test_y <- test$y

```

```{r}

# SVM

# SVM with Radial method
set.seed(100)
indx <- createFolds(train_y, returnTrain = TRUE)
ctrl <- trainControl(method = "cv", index = indx)

svmRTune <- train(x = train_X, y = train_y,
                  method = "svmRadial",
                  preProc = c("center", "scale"),
                  tuneLength = 10,
                  trControl = ctrl)
svmR_pred <- predict(svmRTune, test_X)
resample_svmR <- postResample(svmR_pred, test_y)


plot(svmRTune, scales = list(x = list(log = 2))) 

```

```{r}

#k-NN
set.seed(100)
knnGrid <- data.frame(k = 1:10)
knnTune <- train(x = train_X, y = train_y,
                 method = "knn",
                 preProc = c("center", "scale"),
                 tuneGrid = knnGrid,
                 trControl = ctrl)
                 
plot(knnTune)
knn_pred <- predict(knnTune, test_X)
resample_knn <- postResample(knn_pred, test_y)

```

```{r}

eval_results <- data.frame(rbind("SVMR" = resample_svmR, "k-NN" = resample_knn))
eval_results

```

```{r}

# plot the top 10 important features from SVMR model
svmRImp <- varImp(svmRTune, scale = FALSE)
plot(svmRImp, top = 10)

```

```{r}

# plot the top 10 important features from k-NN model
knnRImp <- varImp(knnTune, scale = FALSE)
plot(knnRImp, top = 10)

```