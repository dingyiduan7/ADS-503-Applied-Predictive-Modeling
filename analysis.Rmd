---
title: "ADS 503 Team 4: Predicting Article Shares"
output: html_notebook
---

# Load Data
```{r}
df <- read.csv("Datasets/OnlineNewsPopularity.csv")
dim(df)
summary(df)
```
The dataset has 39,644 samples and 61 variables. 

# Missing Data
```{r}
sum(is.na(df))
```

None of the variables is missing any data so there is no need for imputation. 

# Define Target and Predictors 
```{r}
df <- df[, names(df) != "url"] # drop the url column, which functions as a sample ID
y <- df$shares # shares is the target variable 
X <- df[, names(df) != "shares"] 
```

# Target Distribution 
```{r}
summary(y)
par(mfrow = c(1, 2))
hist(y)
boxplot(y)
```

The target distribution is highly right-skewed. Let's try log transforming it. 
```{r}
summary(log(y))
par(mfrow = c(1, 2))
hist(log(y)) 
boxplot(log(y))
```

The log of the target is much less skewed. Let's use the log-transformed version. 
```{r}
y <- log(y)
```

## Target-Predictor Correlations
```{r}
library(tidyr)
library(dplyr)
library(ggplot2)
x <- names(X)
correlations <- data.frame(x)
correlations$r <- apply(correlations, 1, function(row) cor(X[, row["x"]], y)) # correlation between each predictor and the target
correlations <- correlations %>% 
  dplyr::arrange(r) %>% 
  dplyr::mutate(x = forcats::fct_inorder(x))
ggplot(correlations) + 
  geom_col(aes(x = x, y = r)) + 
  coord_flip()
```

None of the target-predictor correlations is very strong. The strongest positive correlation is with kw_avg_avg, while the strongest negative correlation is with LDA_02. 
```{r}
par(mfrow = c(1, 2))

plot(X[, "kw_avg_avg"], y, xlab = "kw_avg_avg")
lines(lowess(X[, "kw_avg_avg"], y), col = "red")

plot(X[, "LDA_02"], y, xlab = "LDA_02")
lines(lowess(X[, "LDA_02"], y), col = "red")
```

# Predictor Distributions 
## Near-Zero Variance 
```{r}
library(caret)
degen.cols <- nearZeroVar(X)
for (i in degen.cols) {
  col.name <- names(X)[i]
  hist(X[, col.name], main = col.name, xlab = col.name)
}
```

kw_min_max has near-zero variance so let's remove it. 
```{r}
X <- X[, -degen.cols]
```

## Skew 
```{r}
par(mfrow = c(3, 3))
for (i in 1:ncol(X)) {
  hist(
    X[, i],
    main = NULL,
    xlab = colnames(X)[i]
  )
}
```

The predictors have widely varying ranges and many are quite skewed. We'll need to center and scale them before modeling. 

## Correlations 
### Binary Predictor Correlations 
```{r}
library(corrplot)
X.bi <- Filter(function(x) all(x %in% c(0, 1)), X)
X.non.bi <- X[, !names(X) %in% names(X.bi)]
cor.bi <- cor(X.bi)
corrplot(
  cor.bi,
  method = "square",
  type = "upper",
  order = "hclust"
)
```

The only strong correlations among the binary predictors are between weekday_is_sunday and is_weekend and between weekday_is_saturday and is_weekend, which makes sense. 
```{r}
high.cor.bi <- findCorrelation(cor.bi, cutoff = .7) 
names(X.bi)[high.cor.bi]
X.bi <- X.bi[, -high.cor.bi] # remove is_weekend
cor.bi.filtered <- cor(X.bi)
corrplot(
  cor.bi.filtered,
  method = "square",
  type = "upper"
)
```

Remove weekday_is_sunday to avoid the dummy variable trap. Let's also check the data channel binary predictors for the dummy variable trap. 
```{r}
X.bi <- X.bi[, names(X.bi) != "weekday_is_sunday"]
channels <- c(
  "data_channel_is_world",
  "data_channel_is_socmed",
  "data_channel_is_tech",
  "data_channel_is_bus",
  "data_channel_is_entertainment",
  "data_channel_is_lifestyle"
)
summary(apply(X[, channels], 1, sum)) # check if they always sum to 1 for every sample
```
The channel predictors do not always sum to 1, so they do not fall into the dummy variable trap. 

### Non-Binary Predictor Correlations
```{r}
cor.non.bi <- cor(X.non.bi)
corrplot(
  cor.non.bi,
  method = "square",
  type = "upper",
  tl.pos = "n"
)
```

We can see some strong positive and negative correlations between the non-binary predictors. 
```{r}
high.cor.non.bi <- findCorrelation(cor.non.bi, cutoff = .7)
length(high.cor.non.bi)
names(X.non.bi)[high.cor.non.bi]
```
12 variables contribute to high pairwise correlation between the non-binary predictors. Let's remove them 
```{r}
X.non.bi <- X.non.bi[, -high.cor.non.bi]
cor.non.bi.filtered <- cor(X.non.bi)
corrplot(
  cor.non.bi.filtered,
  method = "square",
  type = "upper",
  tl.pos = "n"
)
```

Let's update our master predictor matrix to only retain the non-problematic predictors. 
```{r}
cols.to.keep <- c(names(X.bi), names(X.non.bi))
X <- X[, names(X) %in% cols.to.keep]
```

# Split the Data 
```{r}
set.seed(100)
train.rows <- createDataPartition(y, p = .8, list = FALSE)
y.train <- y[train.rows]
y.test <- y[-train.rows]
X.train <- X[train.rows, ]
X.test <- X[-train.rows, ]
idx <- createFolds(y.train, returnTrain = TRUE)
ctrl <- trainControl(method = "cv", index = idx)
```

# Modeling
## Linear Models 
### Elastic Net
```{r}
enetGrid <- expand.grid(lambda = c(0, 0.01, .1),
                        fraction = seq(0, 1, length = 10))
set.seed(100)
enet <- train(x = X.train, y = y.train,
                         method = "enet",
                         tuneGrid = enetGrid,
                         trControl = ctrl,
                         preProc = c("center", "scale"))
enet
plot(enet)
```

```{r}
library(Metrics)
pred.enet <- predict(enet, X.test)
resid.enet <- pred.enet - y.test
rmse(y.test, pred.enet)
par(mfrow = c(2, 2))
hist(pred.enet)
plot(y.test, pred.enet)
lines(lowess(y.test, pred.enet), col = "red")
hist(resid.enet)
```

```{r}
plot.var.imp <- function(model) {
  imp <- varImp(model)$importance 
  names(imp) <- "importance"
  df <- imp %>%
    tibble::rownames_to_column() %>% 
    dplyr::rename("variable" = rowname) %>% 
    dplyr::arrange(importance) %>% 
    dplyr::mutate(variable = forcats::fct_inorder(variable))
  ggplot(df) + 
    geom_col(aes(x = variable, y = importance)) + 
    coord_flip()
}
plot.var.imp(enet)
```

### Principal Component Regression (PCR)
```{r}
set.seed(100)
pcr <- train(
  x = X.train,
  y = y.train,
  preProc = c("center", "scale"),
  method = "pcr",
  tuneGrid = expand.grid(ncomp = 1:30),
  trControl = ctrl
)
pcr
plot(pcr)
```

```{r}
pred.pcr <- predict(pcr, X.test)
resid.pcr <- pred.pcr - y.test
rmse(y.test, pred.pcr)
par(mfrow = c(2, 2))
hist(pred.pcr)
plot(y.test, pred.pcr)
lines(lowess(y.test, pred.pcr), col = "red")
hist(resid.pcr)
plot.var.imp(pcr)
```

### Principal Least Squares (PLS)
```{r}
set.seed(100)
pls <- train(
  x = X.train,
  y = y.train,
  preProc = c("center", "scale"),
  method = "pls",
  tuneGrid = expand.grid(ncomp = 1:30),
  trControl = ctrl
)
pls
plot(pls)
```

```{r}
pred.pls <- predict(pls, X.test)
resid.pls <- pred.pls - y.test
rmse(y.test, pred.pls)
par(mfrow = c(2, 2))
hist(pred.pls)
plot(y.test, pred.pls)
lines(lowess(y.test, pred.pls), col = "red")
hist(resid.pls)
plot.var.imp(pls)
```

## Non-Linear Models 
### K-Nearest Neighbors (k-NN)
```{r}
set.seed(100)
knn <- train(
  x = X.train,
  y = y.train,
  preProc = c("center", "scale"),
  method = "knn",
  tuneGrid = expand.grid(k = 1:10),
  trControl = ctrl
)
knn
plot(knn)
```

```{r}
pred.knn <- predict(knn, X.test)
resid.knn <- pred.knn - y.test
rmse(y.test, pred.knn)
par(mfrow = c(2, 2))
hist(pred.knn)
plot(y.test, pred.knn)
lines(lowess(y.test, pred.knn), col = "red")
hist(resid.knn)
plot.var.imp(knn)
```

### Classification and Regression Tree (CART)
```{r}
set.seed(100)
cart <- train(
  x = X.train,
  y = y.train,
  preProc = c("center", "scale"),
  method = "rpart",
  tuneGrid = expand.grid(cp = seq(0.001, 0.1, 0.01))
)
cart 
plot(cart)
```

```{r}
pred.cart <- predict(cart, X.test)
resid.cart <- pred.cart - y.test
rmse(y.test, pred.cart)
par(mfrow = c(2, 2))
hist(pred.cart)
plot(y.test, pred.cart)
lines(lowess(y.test, pred.cart), col = "red")
hist(resid.cart)
plot.var.imp(cart)
```

### Stochastic Gradient Boosting 
```{r}
set.seed(100)
gbm.grid <- expand.grid(n.trees = 100, interaction.depth = 1:5, shrinkage = c(0.01, 0.1), n.minobsinnode = 10)
gbm <- train(
  x = X.train,
  y = y.train,
  trControl = ctrl,
  preProc = c("center", "scale"),
  method = "gbm",
  tuneGrid = gbm.grid
)
gbm
plot(gbm)
```

```{r}
pred.gbm <- predict(gbm, X.test)
resid.gbm <- pred.gbm - y.test
rmse(y.test, pred.gbm)
par(mfrow = c(2, 2))
hist(pred.gbm)
plot(y.test, pred.gbm)
lines(lowess(y.test, pred.gbm), col = "red")
hist(resid.gbm)
```

```{r}
library(gbm)
gbm.imp <- summary.gbm(gbm$finalModel, plotit = FALSE)[1:20, ]
names(gbm.imp) <- c("variable", "importance")
gbm.imp.df <- gbm.imp %>% 
  dplyr::arrange(importance) %>%
  dplyr::mutate(variable = forcats::fct_inorder(variable))
ggplot(gbm.imp.df) + 
  geom_col(aes(x = variable, y = importance)) + 
  coord_flip()
```

# Compare Model Performance
```{r}
# create dataframe to contain all model results 
models <- c("enet", "pcr", "pls", "knn", "cart", "gbm")
results <- data.frame(row.names = models)
results$RMSE.cv <- NA
results$RMSE.cv.sd <- NA
results$RMSE.test <- NA

# add enet results 
results.enet <- enet$results %>% 
  filter(lambda == enet$bestTune$lambda & fraction == enet$bestTune$fraction) %>% 
  select(RMSE, RMSESD)

results["enet", "RMSE.cv"] <- results.enet$RMSE
results["enet", "RMSE.cv.sd"] <- results.enet$RMSESD
results["enet", "RMSE.test"] <- rmse(y.test, pred.enet)

# add PCR results 
results.pcr <- pcr$results %>% 
  filter(ncomp == pcr$bestTune$ncomp) %>% 
  select(RMSE, RMSESD) 

results["pcr", "RMSE.cv"] <- results.pcr$RMSE 
results["pcr", "RMSE.cv.sd"] <- results.pcr$RMSESD
results["pcr", "RMSE.test"] <- rmse(y.test, pred.pcr)

# add PLS results 
results.pls <- pls$results %>% 
  filter(ncomp == pls$bestTune$ncomp) %>% 
  select(RMSE, RMSESD)

results["pls", "RMSE.cv"] <- results.pls$RMSE 
results["pls", "RMSE.cv.sd"] <- results.pls$RMSESD
results["pls", "RMSE.test"] <- rmse(y.test, pred.pls)

# add KNN results 
results.knn <- knn$results %>% 
  filter(k == knn$bestTune$k) %>%
  select(RMSE, RMSESD)

results["knn", "RMSE.cv"] <- results.knn$RMSE
results["knn", "RMSE.cv.sd"] <- results.knn$RMSESD
results["knn", "RMSE.test"] <- rmse(y.test, pred.knn)

# add CART results 
results.cart <- cart$results %>% 
  filter(cp == cart$bestTune$cp) %>% 
  select(RMSE, RMSESD)

results["cart", "RMSE.cv"] <- results.cart$RMSE
results["cart", "RMSE.cv.sd"] <- results.cart$RMSESD
results["cart", "RMSE.test"] <- rmse(y.test, pred.cart)

# add GBM results 
results.gbm <- gbm$results %>% 
  filter(shrinkage == gbm$bestTune$shrinkage & interaction.depth == gbm$bestTune$interaction.depth) %>% 
  select(RMSE, RMSESD)

results["gbm", "RMSE.cv"] <- results.gbm$RMSE
results["gbm", "RMSE.cv.sd"] <- results.gbm$RMSESD
results["gbm", "RMSE.test"] <- rmse(y.test, pred.gbm)

results
```
